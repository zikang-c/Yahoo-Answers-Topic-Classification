{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T22:47:07.990296Z",
     "start_time": "2024-11-19T22:47:07.525474Z"
    }
   },
   "source": [
    "# Import variables from another Jupyter Notebook\n",
    "import nbformat\n",
    "from nbformat import read\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T22:47:56.875681Z",
     "start_time": "2024-11-19T22:47:56.058393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-split training and validation variables from a pickle file\n",
    "with open('split_data_v2.pkl', 'rb') as f:\n",
    "    X_train, X_val, y_train, y_val = pickle.load(f)"
   ],
   "id": "a278494ef3b60873",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T22:48:01.590927Z",
     "start_time": "2024-11-19T22:48:01.224792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define different text feature sets\n",
    "feature_sets = {\n",
    "    'question': X_train['question'],\n",
    "    'best_answer': X_train['best_answer'],\n",
    "    'combined': X_train['question'] + ' ' + X_train['best_answer']\n",
    "}"
   ],
   "id": "5ce19a1f62fff00b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T22:49:09.878431Z",
     "start_time": "2024-11-19T22:49:09.874155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: TF-IDF Encoding for Different Feature Sets and TF-IDF Methods\n",
    "def tfidf_transform(X_train, X_val):\n",
    "    tfidf_methods = {\n",
    "        'TF-IDF (ngram_range=(1,1))': TfidfVectorizer(max_features=10000, ngram_range=(1, 1)),\n",
    "        'TF-IDF (ngram_range=(1,2))': TfidfVectorizer(max_features=10000, ngram_range=(1, 2)),\n",
    "        'TF-IDF (sublinear_tf=True)': TfidfVectorizer(max_features=10000, sublinear_tf=True)\n",
    "    }\n",
    "\n",
    "    encoded_sets = {}\n",
    "    for feature_name, X_train_text in feature_sets.items():\n",
    "        X_val_text = X_val[feature_name] if feature_name in X_val else X_val['question'] + ' ' + X_val['best_answer']\n",
    "\n",
    "        for tfidf_name, tfidf in tfidf_methods.items():\n",
    "            # Apply TF-IDF transformation\n",
    "            X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "            X_val_tfidf = tfidf.transform(X_val_text)\n",
    "\n",
    "            # Store encoded sets\n",
    "            encoded_sets[(feature_name, tfidf_name)] = (X_train_tfidf, X_val_tfidf)\n",
    "\n",
    "    # Save encoded sets to a file for later use\n",
    "    with open('encoded_sets_v2.pkl', 'wb') as f:\n",
    "        pickle.dump(encoded_sets, f)"
   ],
   "id": "1dfdec5bd0629f0f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T23:00:46.341034Z",
     "start_time": "2024-11-19T22:49:13.217540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply TF-IDF transformation and save encoded sets\n",
    "tfidf_transform(X_train, X_val)"
   ],
   "id": "9f4f4b9e8d410b69",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T23:03:28.098012Z",
     "start_time": "2024-11-19T23:03:28.077118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Classifier Training and Evaluation Function\n",
    "def classifier_training_and_evaluation(encoded_file, y_train, y_validation):\n",
    "    # Load encoded sets from file\n",
    "    with open(encoded_file, 'rb') as f:\n",
    "        encoded_sets = pickle.load(f)\n",
    "\n",
    "    classifiers = {\n",
    "        'Random Classifier': DummyClassifier(strategy='uniform'),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, solver='saga'),\n",
    "        'Random Forest': RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    for classifier_name, clf in classifiers.items():\n",
    "        results = []\n",
    "        for (feature_name, tfidf_name), (X_train_tfidf, X_validation_tfidf) in encoded_sets.items():\n",
    "            print(\n",
    "                f'Starting training for {classifier_name} with Feature Set: {feature_name}, TF-IDF Method: {tfidf_name}')\n",
    "\n",
    "            # Create and train pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('clf', clf)\n",
    "            ])\n",
    "            pipeline.fit(X_train_tfidf, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = pipeline.predict(X_validation_tfidf)\n",
    "\n",
    "            # Detailed Classification Report\n",
    "            report = classification_report(y_validation, y_pred, output_dict=True)\n",
    "            print(\n",
    "                f'Classification Report for {classifier_name} with Feature Set: {feature_name}, TF-IDF Method: {tfidf_name}:\\n{classification_report(y_validation, y_pred)}')\n",
    "\n",
    "            # Extract F1 scores for each class\n",
    "            f1_scores_per_class = {f'F1 Score (Class {label})': report[label]['f1-score'] for label in report if\n",
    "                                   label.isdigit()}\n",
    "            f1_scores_per_class['Feature Set'] = feature_name\n",
    "            f1_scores_per_class['TF-IDF Method'] = tfidf_name\n",
    "            f1_scores_per_class['Model'] = classifier_name\n",
    "            results.append(f1_scores_per_class)\n",
    "\n",
    "            print(\n",
    "                f'{feature_name}, {tfidf_name}, {classifier_name} finished: Weighted F1 score is {report.get(\"weighted avg\", {}).get(\"f1-score\")}')\n",
    "\n",
    "        # Display results as a DataFrame for each classifier\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(f'F1 Scores for {classifier_name}:')\n",
    "        print(results_df)"
   ],
   "id": "5f585734977a2c37",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-19T23:03:33.069977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train and evaluate classifiers with different feature sets and TF-IDF methods\n",
    "classifier_training_and_evaluation('encoded_sets_v2.pkl', y_train, y_val)"
   ],
   "id": "9a1abfdaaa11e33e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for Random Classifier with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Random Classifier with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.09      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "question, TF-IDF (ngram_range=(1,1)), Random Classifier finished: Weighted F1 score is 0.09966611330267833\n",
      "Starting training for Random Classifier with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Random Classifier with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "question, TF-IDF (ngram_range=(1,2)), Random Classifier finished: Weighted F1 score is 0.0994919074505103\n",
      "Starting training for Random Classifier with Feature Set: question, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Random Classifier with Feature Set: question, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "question, TF-IDF (sublinear_tf=True), Random Classifier finished: Weighted F1 score is 0.10005415144073355\n",
      "Starting training for Random Classifier with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Random Classifier with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "best_answer, TF-IDF (ngram_range=(1,1)), Random Classifier finished: Weighted F1 score is 0.09962754189169523\n",
      "Starting training for Random Classifier with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Random Classifier with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "best_answer, TF-IDF (ngram_range=(1,2)), Random Classifier finished: Weighted F1 score is 0.09967898565997114\n",
      "Starting training for Random Classifier with Feature Set: best_answer, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Random Classifier with Feature Set: best_answer, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "best_answer, TF-IDF (sublinear_tf=True), Random Classifier finished: Weighted F1 score is 0.10044524811859386\n",
      "Starting training for Random Classifier with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Random Classifier with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "combined, TF-IDF (ngram_range=(1,1)), Random Classifier finished: Weighted F1 score is 0.10060776892524671\n",
      "Starting training for Random Classifier with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Random Classifier with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.09      0.10      0.09     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "combined, TF-IDF (ngram_range=(1,2)), Random Classifier finished: Weighted F1 score is 0.09945582791924461\n",
      "Starting training for Random Classifier with Feature Set: combined, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Random Classifier with Feature Set: combined, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     27495\n",
      "           1       0.10      0.10      0.10     27713\n",
      "           2       0.10      0.10      0.10     27209\n",
      "           3       0.10      0.10      0.10     27038\n",
      "           4       0.10      0.10      0.10     26189\n",
      "           5       0.10      0.10      0.10     27562\n",
      "           6       0.10      0.10      0.10     27104\n",
      "           7       0.10      0.10      0.10     27005\n",
      "           8       0.10      0.10      0.10     26571\n",
      "           9       0.10      0.10      0.10     27434\n",
      "\n",
      "    accuracy                           0.10    271320\n",
      "   macro avg       0.10      0.10      0.10    271320\n",
      "weighted avg       0.10      0.10      0.10    271320\n",
      "\n",
      "combined, TF-IDF (sublinear_tf=True), Random Classifier finished: Weighted F1 score is 0.09944259107500256\n",
      "F1 Scores for Random Classifier:\n",
      "   F1 Score (Class 0)  F1 Score (Class 1)  F1 Score (Class 2)  \\\n",
      "0            0.098858            0.098058            0.102366   \n",
      "1            0.097623            0.100985            0.098687   \n",
      "2            0.103854            0.099942            0.100470   \n",
      "3            0.099809            0.099525            0.099211   \n",
      "4            0.099362            0.097767            0.101012   \n",
      "5            0.100655            0.098920            0.100480   \n",
      "6            0.099808            0.101411            0.100857   \n",
      "7            0.103103            0.100160            0.100059   \n",
      "8            0.099324            0.101044            0.096949   \n",
      "\n",
      "   F1 Score (Class 3)  F1 Score (Class 4)  F1 Score (Class 5)  \\\n",
      "0            0.098853            0.097002            0.099080   \n",
      "1            0.100486            0.099721            0.100333   \n",
      "2            0.098930            0.096670            0.100007   \n",
      "3            0.099026            0.099910            0.103259   \n",
      "4            0.101250            0.100932            0.100529   \n",
      "5            0.099566            0.100066            0.100918   \n",
      "6            0.100573            0.098662            0.098574   \n",
      "7            0.096788            0.094322            0.100885   \n",
      "8            0.100778            0.097759            0.101966   \n",
      "\n",
      "   F1 Score (Class 6)  F1 Score (Class 7)  F1 Score (Class 8)  \\\n",
      "0            0.101567            0.096837            0.101809   \n",
      "1            0.099887            0.099691            0.099018   \n",
      "2            0.101632            0.098808            0.096219   \n",
      "3            0.098287            0.102233            0.097232   \n",
      "4            0.098156            0.098361            0.097673   \n",
      "5            0.101465            0.101378            0.098417   \n",
      "6            0.103920            0.098190            0.102199   \n",
      "7            0.100363            0.098745            0.098358   \n",
      "8            0.099432            0.098944            0.098974   \n",
      "\n",
      "   F1 Score (Class 9)  Feature Set               TF-IDF Method  \\\n",
      "0            0.102189     question  TF-IDF (ngram_range=(1,1))   \n",
      "1            0.098485     question  TF-IDF (ngram_range=(1,2))   \n",
      "2            0.103715     question  TF-IDF (sublinear_tf=True)   \n",
      "3            0.097718  best_answer  TF-IDF (ngram_range=(1,1))   \n",
      "4            0.101751  best_answer  TF-IDF (ngram_range=(1,2))   \n",
      "5            0.102534  best_answer  TF-IDF (sublinear_tf=True)   \n",
      "6            0.101852     combined  TF-IDF (ngram_range=(1,1))   \n",
      "7            0.101450     combined  TF-IDF (ngram_range=(1,2))   \n",
      "8            0.099127     combined  TF-IDF (sublinear_tf=True)   \n",
      "\n",
      "               Model  \n",
      "0  Random Classifier  \n",
      "1  Random Classifier  \n",
      "2  Random Classifier  \n",
      "3  Random Classifier  \n",
      "4  Random Classifier  \n",
      "5  Random Classifier  \n",
      "6  Random Classifier  \n",
      "7  Random Classifier  \n",
      "8  Random Classifier  \n",
      "Starting training for Naive Bayes with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Naive Bayes with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.56     27495\n",
      "           1       0.63      0.70      0.66     27713\n",
      "           2       0.71      0.76      0.73     27209\n",
      "           3       0.55      0.47      0.51     27038\n",
      "           4       0.79      0.83      0.81     26189\n",
      "           5       0.82      0.80      0.81     27562\n",
      "           6       0.56      0.47      0.51     27104\n",
      "           7       0.66      0.65      0.66     27005\n",
      "           8       0.62      0.77      0.69     26571\n",
      "           9       0.73      0.71      0.72     27434\n",
      "\n",
      "    accuracy                           0.67    271320\n",
      "   macro avg       0.66      0.67      0.66    271320\n",
      "weighted avg       0.66      0.67      0.66    271320\n",
      "\n",
      "question, TF-IDF (ngram_range=(1,1)), Naive Bayes finished: Weighted F1 score is 0.6641467853391552\n",
      "Starting training for Naive Bayes with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Naive Bayes with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54     27495\n",
      "           1       0.61      0.69      0.64     27713\n",
      "           2       0.70      0.74      0.72     27209\n",
      "           3       0.53      0.47      0.50     27038\n",
      "           4       0.78      0.82      0.80     26189\n",
      "           5       0.81      0.77      0.79     27562\n",
      "           6       0.55      0.46      0.50     27104\n",
      "           7       0.64      0.63      0.63     27005\n",
      "           8       0.63      0.75      0.68     26571\n",
      "           9       0.72      0.69      0.71     27434\n",
      "\n",
      "    accuracy                           0.65    271320\n",
      "   macro avg       0.65      0.65      0.65    271320\n",
      "weighted avg       0.65      0.65      0.65    271320\n",
      "\n",
      "question, TF-IDF (ngram_range=(1,2)), Naive Bayes finished: Weighted F1 score is 0.6511434345692453\n",
      "Starting training for Naive Bayes with Feature Set: question, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Naive Bayes with Feature Set: question, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.56     27495\n",
      "           1       0.63      0.70      0.66     27713\n",
      "           2       0.71      0.76      0.73     27209\n",
      "           3       0.55      0.47      0.51     27038\n",
      "           4       0.79      0.83      0.81     26189\n",
      "           5       0.82      0.80      0.81     27562\n",
      "           6       0.56      0.47      0.51     27104\n",
      "           7       0.66      0.65      0.66     27005\n",
      "           8       0.62      0.77      0.69     26571\n",
      "           9       0.73      0.71      0.72     27434\n",
      "\n",
      "    accuracy                           0.67    271320\n",
      "   macro avg       0.66      0.67      0.66    271320\n",
      "weighted avg       0.66      0.67      0.66    271320\n",
      "\n",
      "question, TF-IDF (sublinear_tf=True), Naive Bayes finished: Weighted F1 score is 0.6639612024958137\n",
      "Starting training for Naive Bayes with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Naive Bayes with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.43      0.47     27495\n",
      "           1       0.57      0.66      0.61     27713\n",
      "           2       0.66      0.68      0.67     27209\n",
      "           3       0.48      0.31      0.38     27038\n",
      "           4       0.67      0.77      0.72     26189\n",
      "           5       0.72      0.63      0.67     27562\n",
      "           6       0.49      0.41      0.45     27104\n",
      "           7       0.47      0.50      0.48     27005\n",
      "           8       0.46      0.71      0.56     26571\n",
      "           9       0.65      0.59      0.62     27434\n",
      "\n",
      "    accuracy                           0.57    271320\n",
      "   macro avg       0.57      0.57      0.56    271320\n",
      "weighted avg       0.57      0.57      0.56    271320\n",
      "\n",
      "best_answer, TF-IDF (ngram_range=(1,1)), Naive Bayes finished: Weighted F1 score is 0.5632825566876988\n",
      "Starting training for Naive Bayes with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Naive Bayes with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.43      0.47     27495\n",
      "           1       0.55      0.66      0.60     27713\n",
      "           2       0.66      0.67      0.67     27209\n",
      "           3       0.47      0.31      0.37     27038\n",
      "           4       0.67      0.76      0.71     26189\n",
      "           5       0.71      0.62      0.66     27562\n",
      "           6       0.49      0.40      0.44     27104\n",
      "           7       0.46      0.49      0.47     27005\n",
      "           8       0.47      0.70      0.56     26571\n",
      "           9       0.65      0.58      0.61     27434\n",
      "\n",
      "    accuracy                           0.56    271320\n",
      "   macro avg       0.56      0.56      0.56    271320\n",
      "weighted avg       0.56      0.56      0.56    271320\n",
      "\n",
      "best_answer, TF-IDF (ngram_range=(1,2)), Naive Bayes finished: Weighted F1 score is 0.5560829857735962\n",
      "Starting training for Naive Bayes with Feature Set: best_answer, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Naive Bayes with Feature Set: best_answer, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.43      0.47     27495\n",
      "           1       0.57      0.66      0.61     27713\n",
      "           2       0.66      0.68      0.67     27209\n",
      "           3       0.48      0.31      0.38     27038\n",
      "           4       0.67      0.77      0.72     26189\n",
      "           5       0.72      0.63      0.67     27562\n",
      "           6       0.49      0.41      0.44     27104\n",
      "           7       0.47      0.50      0.48     27005\n",
      "           8       0.46      0.71      0.56     26571\n",
      "           9       0.65      0.59      0.62     27434\n",
      "\n",
      "    accuracy                           0.57    271320\n",
      "   macro avg       0.57      0.57      0.56    271320\n",
      "weighted avg       0.57      0.57      0.56    271320\n",
      "\n",
      "best_answer, TF-IDF (sublinear_tf=True), Naive Bayes finished: Weighted F1 score is 0.5622995312436962\n",
      "Starting training for Naive Bayes with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Naive Bayes with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.53      0.56     27495\n",
      "           1       0.70      0.72      0.71     27713\n",
      "           2       0.73      0.78      0.75     27209\n",
      "           3       0.56      0.44      0.49     27038\n",
      "           4       0.80      0.85      0.83     26189\n",
      "           5       0.85      0.81      0.83     27562\n",
      "           6       0.56      0.50      0.53     27104\n",
      "           7       0.64      0.67      0.65     27005\n",
      "           8       0.61      0.79      0.69     26571\n",
      "           9       0.74      0.73      0.73     27434\n",
      "\n",
      "    accuracy                           0.68    271320\n",
      "   macro avg       0.68      0.68      0.68    271320\n",
      "weighted avg       0.68      0.68      0.68    271320\n",
      "\n",
      "combined, TF-IDF (ngram_range=(1,1)), Naive Bayes finished: Weighted F1 score is 0.6776689855372474\n",
      "Starting training for Naive Bayes with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Naive Bayes with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.56     27495\n",
      "           1       0.69      0.72      0.71     27713\n",
      "           2       0.73      0.77      0.75     27209\n",
      "           3       0.55      0.44      0.49     27038\n",
      "           4       0.80      0.84      0.82     26189\n",
      "           5       0.84      0.79      0.82     27562\n",
      "           6       0.55      0.50      0.52     27104\n",
      "           7       0.62      0.65      0.64     27005\n",
      "           8       0.61      0.78      0.69     26571\n",
      "           9       0.74      0.72      0.73     27434\n",
      "\n",
      "    accuracy                           0.67    271320\n",
      "   macro avg       0.67      0.67      0.67    271320\n",
      "weighted avg       0.67      0.67      0.67    271320\n",
      "\n",
      "combined, TF-IDF (ngram_range=(1,2)), Naive Bayes finished: Weighted F1 score is 0.6706600738796208\n",
      "Starting training for Naive Bayes with Feature Set: combined, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Naive Bayes with Feature Set: combined, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.53      0.56     27495\n",
      "           1       0.70      0.72      0.71     27713\n",
      "           2       0.73      0.78      0.75     27209\n",
      "           3       0.56      0.44      0.49     27038\n",
      "           4       0.80      0.85      0.83     26189\n",
      "           5       0.85      0.81      0.83     27562\n",
      "           6       0.56      0.50      0.53     27104\n",
      "           7       0.64      0.67      0.65     27005\n",
      "           8       0.61      0.79      0.69     26571\n",
      "           9       0.74      0.73      0.73     27434\n",
      "\n",
      "    accuracy                           0.68    271320\n",
      "   macro avg       0.68      0.68      0.68    271320\n",
      "weighted avg       0.68      0.68      0.68    271320\n",
      "\n",
      "combined, TF-IDF (sublinear_tf=True), Naive Bayes finished: Weighted F1 score is 0.6765723184552214\n",
      "F1 Scores for Naive Bayes:\n",
      "   F1 Score (Class 0)  F1 Score (Class 1)  F1 Score (Class 2)  \\\n",
      "0            0.556518            0.661920            0.732059   \n",
      "1            0.543392            0.644787            0.716652   \n",
      "2            0.555344            0.661826            0.732125   \n",
      "3            0.472747            0.610279            0.673163   \n",
      "4            0.466288            0.600311            0.667081   \n",
      "5            0.472064            0.610217            0.671917   \n",
      "6            0.560333            0.711052            0.754606   \n",
      "7            0.556920            0.705695            0.749061   \n",
      "8            0.558862            0.710916            0.754083   \n",
      "\n",
      "   F1 Score (Class 3)  F1 Score (Class 4)  F1 Score (Class 5)  \\\n",
      "0            0.506680            0.808761            0.808947   \n",
      "1            0.496333            0.798907            0.788968   \n",
      "2            0.505979            0.809173            0.808980   \n",
      "3            0.378420            0.717960            0.673081   \n",
      "4            0.371679            0.711930            0.659830   \n",
      "5            0.376316            0.717490            0.672556   \n",
      "6            0.494427            0.825901            0.828582   \n",
      "7            0.488910            0.819977            0.816028   \n",
      "8            0.491609            0.825305            0.828344   \n",
      "\n",
      "   F1 Score (Class 6)  F1 Score (Class 7)  F1 Score (Class 8)  \\\n",
      "0            0.510129            0.655507            0.686012   \n",
      "1            0.501881            0.634142            0.684747   \n",
      "2            0.509952            0.655646            0.685642   \n",
      "3            0.446128            0.484912            0.557288   \n",
      "4            0.440873            0.473897            0.559144   \n",
      "5            0.444561            0.484222            0.555570   \n",
      "6            0.528431            0.652736            0.687841   \n",
      "7            0.521295            0.638049            0.685161   \n",
      "8            0.525493            0.653116            0.685823   \n",
      "\n",
      "   F1 Score (Class 9)  Feature Set               TF-IDF Method        Model  \n",
      "0            0.718067     question  TF-IDF (ngram_range=(1,1))  Naive Bayes  \n",
      "1            0.705290     question  TF-IDF (ngram_range=(1,2))  Naive Bayes  \n",
      "2            0.718100     question  TF-IDF (sublinear_tf=True)  Naive Bayes  \n",
      "3            0.620487  best_answer  TF-IDF (ngram_range=(1,1))  Naive Bayes  \n",
      "4            0.611808  best_answer  TF-IDF (ngram_range=(1,2))  Naive Bayes  \n",
      "5            0.619690  best_answer  TF-IDF (sublinear_tf=True)  Naive Bayes  \n",
      "6            0.734846     combined  TF-IDF (ngram_range=(1,1))  Naive Bayes  \n",
      "7            0.727668     combined  TF-IDF (ngram_range=(1,2))  Naive Bayes  \n",
      "8            0.734199     combined  TF-IDF (sublinear_tf=True)  Naive Bayes  \n",
      "Starting training for Logistic Regression with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Logistic Regression with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58     27495\n",
      "           1       0.62      0.73      0.67     27713\n",
      "           2       0.72      0.77      0.74     27209\n",
      "           3       0.55      0.50      0.52     27038\n",
      "           4       0.81      0.84      0.83     26189\n",
      "           5       0.84      0.83      0.83     27562\n",
      "           6       0.57      0.48      0.52     27104\n",
      "           7       0.68      0.67      0.67     27005\n",
      "           8       0.69      0.75      0.72     26571\n",
      "           9       0.75      0.74      0.74     27434\n",
      "\n",
      "    accuracy                           0.68    271320\n",
      "   macro avg       0.68      0.69      0.68    271320\n",
      "weighted avg       0.68      0.68      0.68    271320\n",
      "\n",
      "question, TF-IDF (ngram_range=(1,1)), Logistic Regression finished: Weighted F1 score is 0.6821815351643266\n",
      "Starting training for Logistic Regression with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Logistic Regression with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.55      0.57     27495\n",
      "           1       0.60      0.72      0.66     27713\n",
      "           2       0.71      0.76      0.73     27209\n",
      "           3       0.54      0.49      0.52     27038\n",
      "           4       0.81      0.83      0.82     26189\n",
      "           5       0.83      0.81      0.82     27562\n",
      "           6       0.57      0.47      0.51     27104\n",
      "           7       0.67      0.66      0.66     27005\n",
      "           8       0.69      0.75      0.72     26571\n",
      "           9       0.74      0.73      0.74     27434\n",
      "\n",
      "    accuracy                           0.68    271320\n",
      "   macro avg       0.67      0.68      0.67    271320\n",
      "weighted avg       0.67      0.68      0.67    271320\n",
      "\n",
      "question, TF-IDF (ngram_range=(1,2)), Logistic Regression finished: Weighted F1 score is 0.6735167294832096\n",
      "Starting training for Logistic Regression with Feature Set: question, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Logistic Regression with Feature Set: question, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58     27495\n",
      "           1       0.62      0.73      0.67     27713\n",
      "           2       0.72      0.77      0.74     27209\n",
      "           3       0.55      0.50      0.52     27038\n",
      "           4       0.81      0.84      0.83     26189\n",
      "           5       0.84      0.83      0.84     27562\n",
      "           6       0.57      0.48      0.52     27104\n",
      "           7       0.68      0.67      0.68     27005\n",
      "           8       0.69      0.75      0.72     26571\n",
      "           9       0.75      0.74      0.74     27434\n",
      "\n",
      "    accuracy                           0.69    271320\n",
      "   macro avg       0.68      0.69      0.68    271320\n",
      "weighted avg       0.68      0.69      0.68    271320\n",
      "\n",
      "question, TF-IDF (sublinear_tf=True), Logistic Regression finished: Weighted F1 score is 0.6829833256926973\n",
      "Starting training for Logistic Regression with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Logistic Regression with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.46      0.48     27495\n",
      "           1       0.63      0.65      0.64     27713\n",
      "           2       0.69      0.69      0.69     27209\n",
      "           3       0.46      0.36      0.40     27038\n",
      "           4       0.72      0.76      0.74     26189\n",
      "           5       0.69      0.68      0.68     27562\n",
      "           6       0.51      0.40      0.45     27104\n",
      "           7       0.43      0.57      0.49     27005\n",
      "           8       0.55      0.66      0.60     26571\n",
      "           9       0.63      0.62      0.63     27434\n",
      "\n",
      "    accuracy                           0.58    271320\n",
      "   macro avg       0.58      0.58      0.58    271320\n",
      "weighted avg       0.58      0.58      0.58    271320\n",
      "\n",
      "best_answer, TF-IDF (ngram_range=(1,1)), Logistic Regression finished: Weighted F1 score is 0.5797113219904833\n",
      "Starting training for Logistic Regression with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Logistic Regression with Feature Set: best_answer, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.45      0.48     27495\n",
      "           1       0.63      0.65      0.64     27713\n",
      "           2       0.69      0.68      0.69     27209\n",
      "           3       0.45      0.35      0.39     27038\n",
      "           4       0.72      0.75      0.73     26189\n",
      "           5       0.68      0.66      0.67     27562\n",
      "           6       0.51      0.39      0.44     27104\n",
      "           7       0.42      0.57      0.48     27005\n",
      "           8       0.55      0.66      0.60     26571\n",
      "           9       0.63      0.61      0.62     27434\n",
      "\n",
      "    accuracy                           0.58    271320\n",
      "   macro avg       0.58      0.58      0.57    271320\n",
      "weighted avg       0.58      0.58      0.57    271320\n",
      "\n",
      "best_answer, TF-IDF (ngram_range=(1,2)), Logistic Regression finished: Weighted F1 score is 0.5746216713439976\n",
      "Starting training for Logistic Regression with Feature Set: best_answer, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Logistic Regression with Feature Set: best_answer, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.46      0.48     27495\n",
      "           1       0.63      0.65      0.64     27713\n",
      "           2       0.69      0.69      0.69     27209\n",
      "           3       0.45      0.36      0.40     27038\n",
      "           4       0.72      0.76      0.74     26189\n",
      "           5       0.69      0.68      0.68     27562\n",
      "           6       0.51      0.40      0.45     27104\n",
      "           7       0.43      0.57      0.49     27005\n",
      "           8       0.56      0.66      0.60     26571\n",
      "           9       0.63      0.62      0.63     27434\n",
      "\n",
      "    accuracy                           0.58    271320\n",
      "   macro avg       0.58      0.58      0.58    271320\n",
      "weighted avg       0.58      0.58      0.58    271320\n",
      "\n",
      "best_answer, TF-IDF (sublinear_tf=True), Logistic Regression finished: Weighted F1 score is 0.5799245416150718\n",
      "Starting training for Logistic Regression with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n",
      "Classification Report for Logistic Regression with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,1)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.58     27495\n",
      "           1       0.69      0.75      0.72     27713\n",
      "           2       0.76      0.79      0.77     27209\n",
      "           3       0.56      0.49      0.52     27038\n",
      "           4       0.83      0.85      0.84     26189\n",
      "           5       0.86      0.86      0.86     27562\n",
      "           6       0.58      0.50      0.53     27104\n",
      "           7       0.67      0.70      0.68     27005\n",
      "           8       0.69      0.76      0.73     26571\n",
      "           9       0.75      0.75      0.75     27434\n",
      "\n",
      "    accuracy                           0.70    271320\n",
      "   macro avg       0.70      0.70      0.70    271320\n",
      "weighted avg       0.70      0.70      0.70    271320\n",
      "\n",
      "combined, TF-IDF (ngram_range=(1,1)), Logistic Regression finished: Weighted F1 score is 0.6993579882726209\n",
      "Starting training for Logistic Regression with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,2))\n",
      "Classification Report for Logistic Regression with Feature Set: combined, TF-IDF Method: TF-IDF (ngram_range=(1,2)):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58     27495\n",
      "           1       0.69      0.75      0.72     27713\n",
      "           2       0.75      0.78      0.77     27209\n",
      "           3       0.56      0.49      0.52     27038\n",
      "           4       0.82      0.85      0.84     26189\n",
      "           5       0.85      0.85      0.85     27562\n",
      "           6       0.57      0.50      0.53     27104\n",
      "           7       0.66      0.69      0.68     27005\n",
      "           8       0.69      0.76      0.73     26571\n",
      "           9       0.75      0.75      0.75     27434\n",
      "\n",
      "    accuracy                           0.70    271320\n",
      "   macro avg       0.69      0.70      0.70    271320\n",
      "weighted avg       0.69      0.70      0.70    271320\n",
      "\n",
      "combined, TF-IDF (ngram_range=(1,2)), Logistic Regression finished: Weighted F1 score is 0.6950567036681169\n",
      "Starting training for Logistic Regression with Feature Set: combined, TF-IDF Method: TF-IDF (sublinear_tf=True)\n",
      "Classification Report for Logistic Regression with Feature Set: combined, TF-IDF Method: TF-IDF (sublinear_tf=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.58     27495\n",
      "           1       0.69      0.75      0.72     27713\n",
      "           2       0.76      0.79      0.77     27209\n",
      "           3       0.56      0.50      0.53     27038\n",
      "           4       0.83      0.86      0.84     26189\n",
      "           5       0.86      0.86      0.86     27562\n",
      "           6       0.58      0.50      0.54     27104\n",
      "           7       0.67      0.70      0.69     27005\n",
      "           8       0.70      0.76      0.73     26571\n",
      "           9       0.75      0.76      0.75     27434\n",
      "\n",
      "    accuracy                           0.70    271320\n",
      "   macro avg       0.70      0.70      0.70    271320\n",
      "weighted avg       0.70      0.70      0.70    271320\n",
      "\n",
      "combined, TF-IDF (sublinear_tf=True), Logistic Regression finished: Weighted F1 score is 0.7006929980281994\n",
      "F1 Scores for Logistic Regression:\n",
      "   F1 Score (Class 0)  F1 Score (Class 1)  F1 Score (Class 2)  \\\n",
      "0            0.577014            0.668930            0.743042   \n",
      "1            0.566669            0.656330            0.733533   \n",
      "2            0.577213            0.669493            0.744466   \n",
      "3            0.483930            0.641980            0.690858   \n",
      "4            0.479678            0.636349            0.686438   \n",
      "5            0.484344            0.640898            0.691015   \n",
      "6            0.581790            0.721688            0.772259   \n",
      "7            0.579663            0.716251            0.767345   \n",
      "8            0.583049            0.721970            0.773279   \n",
      "\n",
      "   F1 Score (Class 3)  F1 Score (Class 4)  F1 Score (Class 5)  \\\n",
      "0            0.522892            0.825141            0.834094   \n",
      "1            0.516420            0.817465            0.819169   \n",
      "2            0.522387            0.825939            0.835412   \n",
      "3            0.401035            0.737171            0.682726   \n",
      "4            0.394483            0.734554            0.670716   \n",
      "5            0.401321            0.737083            0.683695   \n",
      "6            0.524808            0.840838            0.857573   \n",
      "7            0.520740            0.838210            0.849165   \n",
      "8            0.526597            0.842089            0.859420   \n",
      "\n",
      "   F1 Score (Class 6)  F1 Score (Class 7)  F1 Score (Class 8)  \\\n",
      "0            0.518883            0.673569            0.720337   \n",
      "1            0.513266            0.662398            0.718725   \n",
      "2            0.518682            0.675326            0.721714   \n",
      "3            0.446404            0.489214            0.600942   \n",
      "4            0.444449            0.480849            0.599783   \n",
      "5            0.446732            0.489631            0.601583   \n",
      "6            0.533904            0.683543            0.726339   \n",
      "7            0.530496            0.675742            0.725676   \n",
      "8            0.535028            0.686112            0.727586   \n",
      "\n",
      "   F1 Score (Class 9)  Feature Set               TF-IDF Method  \\\n",
      "0            0.741363     question  TF-IDF (ngram_range=(1,1))   \n",
      "1            0.735003     question  TF-IDF (ngram_range=(1,2))   \n",
      "2            0.742659     question  TF-IDF (sublinear_tf=True)   \n",
      "3            0.625081  best_answer  TF-IDF (ngram_range=(1,1))   \n",
      "4            0.621387  best_answer  TF-IDF (ngram_range=(1,2))   \n",
      "5            0.625184  best_answer  TF-IDF (sublinear_tf=True)   \n",
      "6            0.753245     combined  TF-IDF (ngram_range=(1,1))   \n",
      "7            0.749854     combined  TF-IDF (ngram_range=(1,2))   \n",
      "8            0.754228     combined  TF-IDF (sublinear_tf=True)   \n",
      "\n",
      "                 Model  \n",
      "0  Logistic Regression  \n",
      "1  Logistic Regression  \n",
      "2  Logistic Regression  \n",
      "3  Logistic Regression  \n",
      "4  Logistic Regression  \n",
      "5  Logistic Regression  \n",
      "6  Logistic Regression  \n",
      "7  Logistic Regression  \n",
      "8  Logistic Regression  \n",
      "Starting training for Random Forest with Feature Set: question, TF-IDF Method: TF-IDF (ngram_range=(1,1))\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
